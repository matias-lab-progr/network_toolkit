# network_toolkit/analysis_tools.py
"""
M√≥dulo de an√°lisis para Network Toolkit - An√°lisis y procesamiento de resultados
"""

import re
from colorama import Fore, Style

def analyse_ping_output(output, target):
    """
    Analiza la salida del comando ping (Linux/macOS/Windows, ES/EN) y devuelve
    un texto explicativo en espa√±ol con m√©tricas y recomendaciones.
    """
    import re
    import math
    from colorama import Fore, Style

    # Helpers
    def safe_float(x, default=None):
        try:
            return float(x)
        except:
            return default

    def parse_int(x, default=None):
        try:
            return int(x)
        except:
            return default

    def fmt_ms(v):
        if v is None:
            return "N/A"
        return f"{v:.2f} ms"

    # Inicializaci√≥n
    analysis_lines = []
    header = f"{Fore.CYAN}--- AN√ÅLISIS PING: {target} ---{Style.RESET_ALL}"
    analysis_lines.append(header)

    # M√©tricas que trataremos de obtener
    metrics = {
        "sent": None,
        "received": None,
        "lost": None,
        "loss_percent": None,
        "rtt_min": None,
        "rtt_avg": None,
        "rtt_max": None,
        "rtt_mdev": None,
        "rtt_stddev": None,
        "jitter": None,
        "sample_times": [],   # lista de tiempos individuales (ms)
        "ttl": None
    }

    lines = [ln.strip() for ln in output.splitlines() if ln.strip()]

    # 1) Intentar extraer paquetes (Unix: "4 packets transmitted, 4 received, 0% packet loss")
    # Linux/mac pattern
    for ln in lines:
        # packets transmitted, received, packet loss
        m = re.search(r'(\d+)\s+packets\s+transmitted[, ]+\s*(\d+)\s+(?:received|received,)\b.*?(\d+)%', ln, re.IGNORECASE)
        if m:
            metrics["sent"] = parse_int(m.group(1))
            metrics["received"] = parse_int(m.group(2))
            metrics["loss_percent"] = parse_int(m.group(3))
            metrics["lost"] = None if metrics["sent"] is None or metrics["received"] is None else metrics["sent"] - metrics["received"]
            break

    # Windows pattern: "Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),"
    if metrics["sent"] is None:
        for ln in lines:
            m = re.search(r'Sent\s*=\s*(\d+)\s*,\s*Received\s*=\s*(\d+)\s*,\s*Lost\s*=\s*(\d+)', ln, re.IGNORECASE)
            if m:
                metrics["sent"] = parse_int(m.group(1))
                metrics["received"] = parse_int(m.group(2))
                metrics["lost"] = parse_int(m.group(3))
                # buscar % loss si aparece
                m2 = re.search(r'\((\d+)%\s*loss\)|\((\d+)%\s*perdidos\)', ln, re.IGNORECASE)
                if m2:
                    metrics["loss_percent"] = parse_int(m2.group(1) or m2.group(2))
                elif metrics["sent"] is not None:
                    metrics["loss_percent"] = 0 if metrics["lost"] == 0 else round(metrics["lost"] / metrics["sent"] * 100, 2)
                break

    # Si a√∫n no se detect√≥, intentar patr√≥n en espa√±ol "4 paquetes transmitidos, 4 recibidos, 0% perdidos"
    if metrics["sent"] is None:
        for ln in lines:
            m = re.search(r'(\d+)\s+paquetes\s+transmitidos[, ]+\s*(\d+)\s+(?:recibidos|recibido)[, ]+.*?(\d+)%', ln, re.IGNORECASE)
            if m:
                metrics["sent"] = parse_int(m.group(1))
                metrics["received"] = parse_int(m.group(2))
                metrics["loss_percent"] = parse_int(m.group(3))
                metrics["lost"] = None if metrics["sent"] is None or metrics["received"] is None else metrics["sent"] - metrics["received"]
                break

    # 2) Extraer valores RTT (Unix: rtt min/avg/max/mdev = 0.123/0.123/0.123/0.000 ms)
    for ln in lines:
        m = re.search(r'=\s*([\d.]+)\/([\d.]+)\/([\d.]+)\/([\d.]+)\s*ms', ln)  # iputils
        if not m:
            m = re.search(r'round-trip.*=\s*([\d.]+)\/([\d.]+)\/([\d.]+)\/([\d.]+)\s*ms', ln, re.IGNORECASE)
        if m:
            metrics["rtt_min"] = safe_float(m.group(1))
            metrics["rtt_avg"] = safe_float(m.group(2))
            metrics["rtt_max"] = safe_float(m.group(3))
            metrics["rtt_mdev"] = safe_float(m.group(4))
            break

    # 2b) Windows block: look for "Minimum = Xms, Maximum = Yms, Average = Zms"
    if metrics["rtt_avg"] is None:
        for ln in lines:
            m = re.search(r'Minimum\s*=\s*([\d<>]+)ms.*Maximum\s*=\s*([\d<>]+)ms.*Average\s*=\s*([\d<>]+)ms', ln, re.IGNORECASE)
            if m:
                def interpret_ms(s):
                    if '<' in s:
                        return 0.5
                    return safe_float(re.sub(r'[^\d.]','',s))
                metrics["rtt_min"] = interpret_ms(m.group(1))
                metrics["rtt_max"] = interpret_ms(m.group(2))
                metrics["rtt_avg"] = interpret_ms(m.group(3))
                break

    # 3) Si no hay resumen RTT, extraer todos los "time=" de respuestas y calcular estad√≠sticas
    if metrics["rtt_avg"] is None:
        times = []
        for ln in lines:
            # ejemplos: time=0.123 ms  | time=1ms | time<1ms
            m = re.search(r'time[=<]?\s*([\d.]+)\s*ms', ln, re.IGNORECASE)
            if m:
                val = safe_float(m.group(1))
                if val is not None:
                    times.append(val)
                continue
            # Windows format "time<1ms" -> approximate
            if re.search(r'time\s*<\s*1\s*ms', ln, re.IGNORECASE):
                times.append(0.5)
        if times:
            metrics["sample_times"] = times
            metrics["rtt_min"] = min(times)
            metrics["rtt_max"] = max(times)
            metrics["rtt_avg"] = sum(times) / len(times)
            # stddev
            if len(times) > 1:
                mean = metrics["rtt_avg"]
                variance = sum((t - mean) ** 2 for t in times) / len(times)
                metrics["rtt_stddev"] = math.sqrt(variance)
                metrics["rtt_mdev"] = metrics["rtt_stddev"]
            else:
                metrics["rtt_stddev"] = 0.0
                metrics["rtt_mdev"] = 0.0

    # 4) TTL: buscar la primera aparici√≥n de TTL= o ttl=
    for ln in lines:
        m = re.search(r'TTL=(\d+)', ln, re.IGNORECASE)
        if not m:
            m = re.search(r'ttl=(\d+)', ln, re.IGNORECASE)
        if m:
            metrics["ttl"] = parse_int(m.group(1))
            break

    # 5) Si no tenemos loss_percent pero s√≠ sent/received, calcularlo
    if metrics["loss_percent"] is None and metrics["sent"] is not None and metrics["received"] is not None:
        metrics["lost"] = metrics["sent"] - metrics["received"]
        try:
            metrics["loss_percent"] = round(metrics["lost"] / metrics["sent"] * 100, 2) if metrics["sent"] > 0 else None
        except:
            metrics["loss_percent"] = None

    # --- Generar an√°lisis en texto (espa√±ol) ---
    # 1. P√©rdida de paquetes
    lp = metrics["loss_percent"]
    if lp is None:
        analysis_lines.append(f"‚Ä¢ P√©rdida de paquetes: {Fore.YELLOW}No disponible{Style.RESET_ALL}")
    else:
        loss_color = Fore.GREEN if lp == 0 else (Fore.YELLOW if lp <= 2 else (Fore.MAGENTA if lp <= 5 else Fore.RED))
        analysis_lines.append(f"‚Ä¢ P√©rdida de paquetes: {loss_color}{lp}%{Style.RESET_ALL}")
        # Interpretaci√≥n
        if lp == 0:
            analysis_lines.append("  ‚Üí Excelente: no hay p√©rdida de paquetes.")
        elif lp <= 2:
            analysis_lines.append("  ‚Üí Muy buena: p√©rdida m√≠nima, es tolerable.")
        elif lp <= 5:
            analysis_lines.append("  ‚Üí Aceptable pero a vigilar: posible congesti√≥n intermitente.")
        else:
            analysis_lines.append("  ‚Üí Problem√°tica: p√©rdida significativa. Revisar conexi√≥n/ISP/infraestructura.")

    # 2. Latencia (RTT) y jitter/stddev
    if metrics["rtt_avg"] is not None:
        avg = metrics["rtt_avg"]
        maxv = metrics["rtt_max"]
        minv = metrics["rtt_min"]
        stddev = metrics.get("rtt_stddev", metrics.get("rtt_mdev"))
        analysis_lines.append(f"‚Ä¢ Latencia (RTT): min={fmt_ms(minv)}, avg={fmt_ms(avg)}, max={fmt_ms(maxv)}")
        if stddev is not None:
            analysis_lines.append(f"  ‚Üí Jitter/StdDev aproximado: {fmt_ms(stddev)}")
        # Categorizar
        if avg < 50:
            analysis_lines.append(f"  ‚Üí Latencia excelente ({avg:.1f} ms). Adecuada para juegos/videollamadas.")
        elif avg < 100:
            analysis_lines.append(f"  ‚Üí Latencia buena ({avg:.1f} ms). Adecuada para la mayor√≠a de usos.")
        elif avg < 200:
            analysis_lines.append(f"  ‚Üí Latencia moderada ({avg:.1f} ms). Puede afectar aplicaciones en tiempo real.")
        else:
            analysis_lines.append(f"  ‚Üí Latencia alta ({avg:.1f} ms). Revisar ruta/ISP.")

    else:
        analysis_lines.append(f"‚Ä¢ Latencia (RTT): {Fore.YELLOW}No disponible{Style.RESET_ALL}")

    # 3. TTL y deducci√≥n SO aproximada
    if metrics["ttl"] is not None:
        ttl = metrics["ttl"]
        analysis_lines.append(f"‚Ä¢ TTL (Time To Live): {Fore.WHITE}{ttl}{Style.RESET_ALL}")
        if ttl <= 64:
            guess = "Linux/Unix (TTL inicial t√≠pico: 64)"
        elif 65 <= ttl <= 128:
            guess = "Windows (TTL inicial t√≠pico: 128)"
        else:
            guess = "Dispositivo/entorno con TTL alto o salto de red complejo"
        analysis_lines.append(f"  ‚Üí Estimaci√≥n r√°pida del SO/stack: {guess}")
    else:
        analysis_lines.append("‚Ä¢ TTL: No detectado en la salida.")

    # 4. Recomendaciones pr√°cticas (diagn√≥stico)
    analysis_lines.append("\n‚Ä¢ Recomendaciones / siguientes pasos:")
    # Prioritizar seg√∫n m√©tricas
    if lp is not None and lp > 5:
        analysis_lines.append("  - Alta p√©rdida detectada: Ejecutar pruebas desde otro punto de la red (otra m√°quina), comprobar cableado/puerto, y contactar ISP si el problema persiste.")
        analysis_lines.append("  - Ejecutar `mtr <host>` (Linux/macOS) o `pathping <host>` (Windows) para identificar d√≥nde ocurre la p√©rdida.")
        analysis_lines.append("  - Repetir ping con m√°s paquetes: `ping -c 50 <host>` (Linux) / `ping -n 50 <host>` (Windows).")
    elif metrics["rtt_avg"] is not None and metrics["rtt_avg"] > 200:
        analysis_lines.append("  - Latencia muy alta: probar trazas (`traceroute` / `tracert`) y contactar al proveedor si el cuello de botella est√° fuera de la red local.")
    else:
        analysis_lines.append("  - Si observas jitter o picos, realiza pruebas sostenidas (MTR) para localizar el salto problem√°tico.")
        analysis_lines.append("  - Para pruebas m√°s precisas, usar `ping` con payload y tamano: `ping -s 1400 <host>` (Linux).")
    analysis_lines.append("  - Comprueba la carga del equipo local (CPU/uso de NIC) y evita Wi-Fi si buscas diagn√≥stico de calidad de enlace.")
    analysis_lines.append("  - Si es una red empresarial: revisa duplex mismatch y configuraci√≥n del switch hacia el host.")

    # 5. Append raw summary metrics en formato compacto (√∫til para logging)
    analysis_lines.append("\n‚Ä¢ M√©tricas detectadas (resumen):")
    analysis_lines.append(f"  - Sent: {metrics['sent']}, Received: {metrics['received']}, Lost: {metrics['lost']}, Loss%: {metrics['loss_percent']}")
    analysis_lines.append(f"  - RTT min/avg/max (ms): {metrics['rtt_min']}/{metrics['rtt_avg']}/{metrics['rtt_max']}")
    if metrics.get("rtt_stddev") is not None:
        analysis_lines.append(f"  - RTT stddev: {metrics['rtt_stddev']:.2f} ms")

    # Pie
    analysis_lines.append(f"{Fore.CYAN}{'-'*40}{Style.RESET_ALL}")

    return "\n".join(analysis_lines)


def analyze_traceroute_output(output, target):
    """Analiza la salida del comando traceroute y proporciona informaci√≥n detallada."""
    analysis = "\n--- AN√ÅLISIS TRACEROUTE ---\n"
    lines = output.splitlines()
    
    hops = []
    total_hops = 0
    timeout_hops = 0
    private_ips = 0
    max_latency = 0
    slow_hops = []  # Cambiamos a diccionario para agrupar por n√∫mero de salto
    
    def is_private_ip(ip):
        """Verifica si una IP es privada"""
        if not ip or ip == '*' or 'Tiempo' in ip:
            return False
            
        # M√©todo simplificado para verificar IPs privadas
        if ip.startswith('10.') or ip.startswith('192.168.') or \
           (ip.startswith('172.') and 16 <= int(ip.split('.')[1]) <= 31) or \
           ip.startswith('169.254.') or ip.startswith('127.'):
            return True
        return False
    
    # Procesar cada l√≠nea
    for line in lines:
        line = line.strip()
        
        # Saltar l√≠neas vac√≠as o de encabezado
        if not line or 'traceroute' in line.lower() or 'tracing' in line.lower() or 'traza' in line.lower():
            continue
        
        # Buscar l√≠neas que comienzan con n√∫mero (saltos)
        if re.match(r'^\s*\d+\s+', line):
            # Usar expresi√≥n regular para dividir correctamente
            match = re.match(r'^\s*(\d+)\s+([^\s]+)\s+([^\s]+)\s+([^\s]+)\s+(.*)$', line)
            if match:
                try:
                    hop_num = int(match.group(1))
                    time1 = match.group(2)
                    time2 = match.group(3)
                    time3 = match.group(4)
                    host = match.group(5).strip()
                    
                    # Extraer IP si est√° disponible
                    ip_match = re.search(r'(\d+\.\d+\.\d+\.\d+)', host)
                    ip = ip_match.group(1) if ip_match else host
                    
                    # Limpiar el hostname (quitar texto adicional)
                    if ip and ip != host:
                        # Si encontramos una IP, usarla como host principal
                        host = ip
                    
                    # Contar timeouts
                    timeouts = 0
                    for t in [time1, time2, time3]:
                        if t == '*' or 'Tiempo' in t:
                            timeouts += 1
                    
                    hops.append({
                        'hop': hop_num,
                        'times': [time1, time2, time3],
                        'host': host,
                        'ip': ip,
                        'is_private': is_private_ip(ip),
                        'timeouts': timeouts
                    })
                except (ValueError, IndexError):
                    continue
    
    # Analizar los datos recolectados
    total_hops = len(hops)
    
    # Diccionario para almacenar la m√°xima latencia por salto
    max_latency_by_hop = {}
    
    for hop in hops:
        # Contar timeouts
        if hop['timeouts'] > 0:
            timeout_hops += 1
        
        # Contar IPs privadas
        if hop['is_private']:
            private_ips += 1
        
        # Calcular latencia m√°xima por salto y global
        hop_max_latency = 0
        for time_str in hop['times']:
            if time_str != '*' and 'Tiempo' not in time_str:
                try:
                    # Extraer valor num√©rico (eliminar 'ms' y convertir a n√∫mero)
                    latency_str = re.sub(r'[^\d]', '', time_str)
                    if latency_str:  # Asegurarse de que no est√© vac√≠o
                        latency = int(latency_str)
                        if latency > hop_max_latency:
                            hop_max_latency = latency
                        if latency > max_latency:
                            max_latency = latency
                except (ValueError, TypeError):
                    continue
        
        # Almacenar la m√°xima latencia por salto
        max_latency_by_hop[hop['hop']] = {
            'latency': hop_max_latency,
            'host': hop['host']
        }
        
        # Identificar saltos lentos (>100ms)
        if hop_max_latency > 100:
            slow_hops.append({
                'hop': hop['hop'],
                'latency': hop_max_latency,
                'host': hop['host']
            })
    
    # Generar an√°lisis
    analysis += f"‚Ä¢ Saltos totales: {total_hops}\n"
    if total_hops > 0:
        analysis += f"‚Ä¢ Saltos con timeouts: {timeout_hops} ({timeout_hops/total_hops*100:.1f}%)\n"
    analysis += f"‚Ä¢ IPs privadas encontradas: {private_ips}\n"
    analysis += f"‚Ä¢ Latencia m√°xima: {max_latency} ms\n"
    
    # An√°lisis de saltos lentos (agrupados por n√∫mero de salto)
    if slow_hops:
        # Eliminar duplicados y quedarnos con la m√°xima latencia por salto
        unique_slow_hops = {}
        for slow_hop in slow_hops:
            hop_num = slow_hop['hop']
            if hop_num not in unique_slow_hops or slow_hop['latency'] > unique_slow_hops[hop_num]['latency']:
                unique_slow_hops[hop_num] = slow_hop
        
        analysis += f"‚Ä¢ Saltos lentos (>100ms): {len(unique_slow_hops)}\n"
        for slow_hop in sorted(unique_slow_hops.values(), key=lambda x: x['latency'], reverse=True)[:5]:
            analysis += f"  - Salto {slow_hop['hop']}: {slow_hop['latency']}ms ({slow_hop['host']})\n"
    
    # Identificar problemas de red
    if total_hops > 0 and timeout_hops / total_hops > 0.3:
        analysis += "‚ö†Ô∏è  ALTO PORCENTAJE DE TIMEOUTS: Puede haber filtrado de paquetes o problemas de ruteo.\n"
    
    if private_ips > 0:
        analysis += "üîç SE DETECTARON IPs PRIVADAS: La ruta pasa por redes internas/NAT.\n"
    
    # Mostrar informaci√≥n de cada salto
    analysis += "\n‚Ä¢ Detalle de saltos:\n"
    for hop in hops:
        status = "üü¢" if hop['timeouts'] == 0 else "üî¥" if hop['timeouts'] == 3 else "üü°"
        private_flag = " (Privada)" if hop['is_private'] else ""
        
        # Mostrar latencia m√°xima del salto
        latency_info = ""
        if hop['hop'] in max_latency_by_hop and max_latency_by_hop[hop['hop']]['latency'] > 0:
            latency_info = f" [M√°x: {max_latency_by_hop[hop['hop']]['latency']}ms]"
        
        analysis += f"  {status} Salto {hop['hop']}: {hop['host']}{private_flag}{latency_info}\n"
        if hop['timeouts'] > 0:
            analysis += f"     Timeouts: {hop['timeouts']}/3 intentos\n"
    
    analysis += "\n‚Ä¢ Recomendaciones:\n"
    if max_latency > 200:
        analysis += "  - Latencia muy alta. Considerar proveedor de internet alternativo.\n"
    if timeout_hops > 0:
        analysis += "  - Timeouts detectados. Puede indicar filtrado de paquetes o congesti√≥n.\n"
    
    analysis += "--------------------------------\n"
    return analysis

def analyze_whois_output(output, domain):
    #Analiza la salida de WHOIS y proporciona informaci√≥n resumida
    analysis = "\n--- AN√ÅLISIS WHOIS ---\n"
    
    lines = output.split('\n')
    
    # Buscar informaci√≥n importante
    creation_date = None
    expiration_date = None
    updated_date = None
    registrar = None
    name_servers = []
    
    for line in lines:
        line = line.strip()
        
        # Fechas de creaci√≥n (manejar formato datetime)
        if 'Fecha de creaci√≥n:' in line:
            # Buscar patrones de fecha en diferentes formatos
            date_patterns = [
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',  # YYYY-MM-DD
                r'datetime\.datetime\((\d{4}), (\d{1,2}), (\d{1,2})',  # datetime(1997, 9, 15
                r'(\d{4}-\d{2}-\d{2})'  # Formato ISO
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, line)
                if match:
                    if 'datetime' in pattern:
                        # Formato: datetime.datetime(1997, 9, 15, 4, 0)
                        year, month, day = match.groups()[:3]
                        creation_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    else:
                        creation_date = match.group(1)
                    break
        
        # Fecha de expiraci√≥n
        elif 'Fecha de expiraci√≥n:' in line:
            date_patterns = [
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'datetime\.datetime\((\d{4}), (\d{1,2}), (\d{1,2})',
                r'(\d{4}-\d{2}-\d{2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, line)
                if match:
                    if 'datetime' in pattern:
                        year, month, day = match.groups()[:3]
                        expiration_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    else:
                        expiration_date = match.group(1)
                    break
        
        # √öltima actualizaci√≥n
        elif '√öltima actualizaci√≥n:' in line:
            date_patterns = [
                r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})',
                r'datetime\.datetime\((\d{4}), (\d{1,2}), (\d{1,2})',
                r'(\d{4}-\d{2}-\d{2})'
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, line)
                if match:
                    if 'datetime' in pattern:
                        year, month, day = match.groups()[:3]
                        updated_date = f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                    else:
                        updated_date = match.group(1)
                    break
        
        # Registrador
        elif 'Registrador:' in line:
            registrar = line.split('Registrador:')[-1].strip()
        
        # Servidores de nombres
        elif re.match(r'^\s*-\s+[A-Za-z0-9.-]+\.[A-Za-z]{2,}', line):
            ns = line.strip().lstrip('-').strip()
            if ns and ns not in name_servers:
                name_servers.append(ns)
    
    # An√°lisis de fechas
    analysis += "‚Ä¢ Informaci√≥n del dominio:\n"
    if creation_date:
        analysis += f"  - Creaci√≥n: {creation_date}\n"
    if expiration_date:
        analysis += f"  - Expiraci√≥n: {expiration_date}\n"
        # Calcular d√≠as hasta expiraci√≥n
        try:
            from datetime import datetime
            exp_date = datetime.strptime(expiration_date, '%Y-%m-%d')
            days_left = (exp_date - datetime.now()).days
            analysis += f"  - D√≠as hasta expiraci√≥n: {days_left}\n"
            if days_left < 30:
                analysis += "  ‚ö†Ô∏è  ¬°El dominio expira pronto!\n"
            elif days_left > 3650:  # 10 a√±os
                analysis += "  ‚úÖ Dominio registrado por mucho tiempo\n"
        except:
            pass
    if updated_date:
        analysis += f"  - √öltima actualizaci√≥n: {updated_date}\n"
        try:
            from datetime import datetime
            update_date = datetime.strptime(updated_date, '%Y-%m-%d')
            days_since_update = (datetime.now() - update_date).days
            if days_since_update > 365:
                analysis += f"  ‚ö†Ô∏è  Sin actualizaciones hace {days_since_update} d√≠as\n"
        except:
            pass
    
    if registrar:
        analysis += f"‚Ä¢ Registrador: {registrar}\n"
        # An√°lisis del registrador
        if 'markmonitor' in registrar.lower():
            analysis += "  ‚úÖ Registrador profesional (empresas grandes)\n"
        elif 'godaddy' in registrar.lower() or 'namecheap' in registrar.lower():
            analysis += "  ‚ÑπÔ∏è  Registrador popular (uso general)\n"
    
    # An√°lisis de servidores de nombres
    if name_servers:
        analysis += f"‚Ä¢ Servidores DNS ({len(name_servers)}):\n"
        for ns in sorted(name_servers)[:4]:
            analysis += f"  - {ns}\n"
        if len(name_servers) > 4:
            analysis += f"  - ... y {len(name_servers) - 4} m√°s\n"
        
        # Verificar configuraci√≥n DNS
        if len(name_servers) >= 2:
            analysis += "  ‚úÖ Configuraci√≥n redundante (buena pr√°ctica)\n"
        
        # Verificar si usa servidores propios
        domain_clean = domain.lower().replace('www.', '').split('.')[0]
        own_ns = sum(1 for ns in name_servers if domain_clean in ns.lower())
        
        if own_ns >= len(name_servers) / 2:
            analysis += "  ‚úÖ Usa servidores propios (configuraci√≥n profesional)\n"
        else:
            analysis += "  ‚ÑπÔ∏è  Usa servidores de terceros\n"
    
    # Estado del dominio
    analysis += "‚Ä¢ Estado del dominio:\n"
    if creation_date:
        try:
            from datetime import datetime
            create_date = datetime.strptime(creation_date, '%Y-%m-%d')
            domain_age = (datetime.now() - create_date).days // 365
            analysis += f"  - Edad aproximada: {domain_age} a√±os\n"
            if domain_age > 10:
                analysis += "  ‚úÖ Dominio antiguo (mayor confianza)\n"
        except:
            pass
    
    # Recomendaciones de seguridad
    analysis += "\n‚Ä¢ Recomendaciones:\n"
    analysis += "  - Verificar periodicamente los datos WHOIS\n"
    analysis += "  - Considerar protecci√≥n de privacidad del dominio\n"
    analysis += "  - Mantener actualizada la informaci√≥n de contacto\n"
    
    analysis += "--------------------------------\n"
    return analysis

def analyze_dns_output(output, domain):
    """Analiza la salida de DNS Lookup y explica los registros."""
    analysis = "\n--- AN√ÅLISIS DNS ---\n"
    
    lines = output.split('\n')
    
    # Detectar tipos de registros
    ipv4_addresses = []
    ipv6_addresses = []
    name_servers = []
    is_authoritative = "no autoritativa" not in output.lower()
    
    for line in lines:
        line = line.strip()
        
        # Detectar direcciones IPv4 (A records)
        if re.match(r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$', line):
            if line not in ipv4_addresses:
                ipv4_addresses.append(line)
        
        # Detectar direcciones IPv6 (AAAA records) - patrones m√°s espec√≠ficos
        elif re.match(r'^[0-9a-fA-F:]+:[0-9a-fA-F:]+$', line) and line.count(':') >= 2:
            if line not in ipv6_addresses:
                ipv6_addresses.append(line)
        
        # Detectar servidores de nombres en respuestas de nslookup
        elif ('internet address' in line.lower() or 'addresses:' in line.lower()) and \
             not line.startswith('Server:') and not line.startswith('Address:'):
            # Extraer IPs de l√≠neas como "Addresses:  2800:3f0:4003:c03::71"
            ips = re.findall(r'(?:[0-9]{1,3}\.){3}[0-9]{1,3}|[0-9a-fA-F:]+:[0-9a-fA-F:]+', line)
            for ip in ips:
                if ':' in ip and ip not in ipv6_addresses:
                    ipv6_addresses.append(ip)
                elif '.' in ip and ip not in ipv4_addresses:
                    ipv4_addresses.append(ip)
    
    # An√°lisis de autoritatividad
    analysis += f"‚Ä¢ Respuesta: {'Autoritativa' if is_authoritative else 'No autoritativa'}\n"
    if not is_authoritative:
        analysis += "  ‚ÑπÔ∏è  Informaci√≥n desde cach√© DNS local\n"
    
    # An√°lisis de registros A (IPv4)
    if ipv4_addresses:
        analysis += f"‚Ä¢ Registros A (IPv4): {len(ipv4_addresses)} direcciones\n"
        for ip in sorted(ipv4_addresses)[:3]:
            analysis += f"  - {ip}\n"
        if len(ipv4_addresses) > 3:
            analysis += f"  - ... y {len(ipv4_addresses) - 3} m√°s\n"
        
        # An√°lisis de distribuci√≥n de IPs
        if len(ipv4_addresses) > 1:
            analysis += "  ‚úÖ M√∫ltiples IPs (balanceo de carga/geo-distribuci√≥n)\n"
            
            # Verificar si est√°n en el mismo rango
            first_octets = [ip.split('.')[0] for ip in ipv4_addresses]
            if len(set(first_octets)) == 1:
                analysis += "  üìç Mismo rango de IPs (probable mismo datacenter)\n"
            else:
                analysis += "  üåç Diferentes rangos (geo-distribuci√≥n)\n"
    
    # An√°lisis de registros AAAA (IPv6)
    if ipv6_addresses:
        analysis += f"‚Ä¢ Registros AAAA (IPv6): {len(ipv6_addresses)} direcciones\n"
        for ip in sorted(ipv6_addresses)[:2]:
            analysis += f"  - {ip}\n"
        analysis += "  ‚úÖ Soporte para IPv6 (conexiones modernas)\n"
    else:
        analysis += "‚Ä¢ IPv6: No detectado\n"
        analysis += "  ‚ÑπÔ∏è  Considerar implementar IPv6\n"
    
    # An√°lisis de disponibilidad
    analysis += "‚Ä¢ Disponibilidad:\n"
    if ipv4_addresses:
        analysis += "  ‚úÖ Servicio accesible via IPv4\n"
    if ipv6_addresses:
        analysis += "  ‚úÖ Servicio accesible via IPv6\n"
    
    # Recomendaciones
    analysis += "\n‚Ä¢ Recomendaciones:\n"
    if ipv4_addresses and ipv6_addresses:
        analysis += "  ‚úÖ Excelente: Soporte dual-stack (IPv4 + IPv6)\n"
    elif len(ipv4_addresses) >= 3:
        analysis += "  ‚úÖ Bueno: M√∫ltiples IPs IPv4 para redundancia\n"
    
    if len(ipv4_addresses) > 5:
        analysis += "  üöÄ Excelente: Alta disponibilidad con muchas IPs\n"
    
    analysis += "--------------------------------\n"
    return analysis